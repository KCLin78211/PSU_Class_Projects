\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{fullpage} % changes the margin
\usepackage{color,listings,graphicx,float,booktabs,multirow, amsmath}
\usepackage[colorlinks=true, urlcolor=blue]{hyperref}
\usepackage{hyperref}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

\begin{document}
\graphicspath{{./figures/}}
\noindent
\large\textbf{Kyle Salitrik} \\
\normalsize CMPSC 450\\
\large{Homework 4 Report} \hfill 

%---------------------------------------------------------------
% Section 0
%---------------------------------------------------------------
\section*{Serial vs Parallel}
After adapting the CUDA code (ref: \url{https://github.com/amosgwa/Exclusive-Scan-CUDA}, \url{https://developer.nvidia.com/gpugems/GPUGems3/gpugems3_ch39.html}) to handle arrays larger than 1024, the program was set to run for M = 1000 to M = 10,000,000 in steps of 1000, where M is the size of the vector. Times were collected at the following sections in the code:
\begin{itemize}
	\item Before copying array to GPU
	\item Before running the GPU calculations
	\item After running the GPU calculations
	\item After copying the array from the GPU to the host
	\item Before running the serialized code
	\item After running the serialized code
\end{itemize}

This data is plotted in the figure below. Unsurprisingly, at low values of M, the serial computation is significantly faster due to the amount of time that it takes to copy the memory from the host to the GPU. The data ends up slightly noisy and inconsistent using such a fine step, however at around 261000 elements, a clear divergence starts where the GPU calculation time begins to be less even when copying all of the elements is considered. 

In contrast, if one looks at only the prefix sum calculation time, it increases at a significantly slower rate than the CPU runtimes. Looking at the actual percentage increase, the GPU calculation only increases by $\approx54.46x$ while the CPU time increases by $\approx1626x$, with the total GPU execution time increasing by only $\approx13x$. This shows that both the CPU and GPU implementations are bound by computation time after a certain point, where memory access becomes irrelevant. The following figure includes a graph of the runtimes.
\newpage
\begin{figure}[H]
	\centering
	\centerline{\includegraphics[width=10.5in,angle=90]{runtimes.png}}
	\caption{Plot of Runtimes}
\end{figure}





%====================================================================
\newpage
\section*{Code Appendix}
\lstinputlisting[language=C]{../prefix_sum.cu}


\end{document}